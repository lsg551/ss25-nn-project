{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\")) # make src available as a package\n",
    "\n",
    "SEED = int(os.getenv(\"SEED\", 42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94156b",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import mMARCO\n",
    "\n",
    "\n",
    "# NOTE: the current implementation of `collate_fn` in `mMARCO`\n",
    "# doubles each sample in size, so 16*2 = 32\n",
    "batch_size = 16\n",
    "\n",
    "# only used for testing here, use the dataloaders otherwise\n",
    "mmarco = mMARCO(seed=SEED, shuffle_buffer_size=64, shuffle=False)\n",
    "print(mmarco._data.info.splits)\n",
    "\n",
    "for sample in mmarco:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc6b46e",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for Apple's Metal (MPS) backend / framework\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e270df",
   "metadata": {},
   "source": [
    "Pointwise cross-encoders are supposed to output a class label (e.g., *relevant*\n",
    "or *not-relevant*) or a relevance score (e.g., from 0 to 1). Now, this model\n",
    "returns raw logits (practically a relevance score). However, the dataset does\n",
    "not have any gold labels, which could be used to compute a loss.\n",
    "\n",
    "Fortunately, the `(query, positive, negative)` triplets can easily be framed as\n",
    "such:\n",
    "- `(query, positive)=1` now means *relevant*\n",
    "- `(query, negative)=0` now means *not-relevant*\n",
    "\n",
    "Et voilà, this can be used for trainig.\n",
    "\n",
    "During training, we have to apply sigmoid to the raw logits to get a binary\n",
    "score though. However, during inference, we want the raw logits to build a\n",
    "sorted ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ad00e",
   "metadata": {},
   "source": [
    "NOTE: There are quite a few CEL functions...\n",
    "- `nn.CrossEntropyLoss` \n",
    "  - **use case**: multi-class classification\n",
    "  - **input**: raw logits for each class (`input`), and the target class indices (`target`).\n",
    "- `nn.BCELoss`\n",
    "  - **use case**: binary classification or multi-label classification (independent classes)\n",
    "  - **input**: probabilities (so sigmoid must already be applied) and the target class indices (`target`).\n",
    "- `nn.BCEWithLogitsLoss` ← ✅\n",
    "  - **use case**: binary classification or multi-label classification (independent classes)\n",
    "  - **input**: raw logits (so sigmoid is applied internally), and the target class indices (`target`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55932bc4",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import CrossEncoderBERT\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "padding = \"longest\"\n",
    "max_length = 256\n",
    "\n",
    "model = CrossEncoderBERT(\n",
    "    enable_gradient_checkpointing=True,\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 2e-5\n",
    "epochs = 3\n",
    "# epochs = 20\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train_dl, val_dl, test_dl = mmarco.as_dataloaders(seed=SEED, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf145e6",
   "metadata": {},
   "source": [
    "### Progress Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae3ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Formatter\n",
    "import ipywidgets as widgets\n",
    "\n",
    "progress_widget = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=epochs,\n",
    "    description=f\"Epoch: 0/{epochs}\",\n",
    ")\n",
    "\n",
    "def update_progress(epoch: int):\n",
    "    progress_widget.value = epoch + 1\n",
    "    progress_widget.description = f\"Epoch: {epoch + 1}/{epochs}\"\n",
    "    \n",
    "\n",
    "training_loss_widget = widgets.FloatProgress(\n",
    "    value=1,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    description=\"Train: 1.000\",\n",
    "    bar_style=\"\",\n",
    "    style={\"bar_color\": \"darkred\"},\n",
    ")\n",
    "\n",
    "val_loss_widget = widgets.FloatProgress(\n",
    "    value=1,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    description=\"Val: 1.000\",\n",
    "    bar_style=\"\",\n",
    "    style={\"bar_color\": \"darkred\"},\n",
    ")\n",
    "\n",
    "\n",
    "def update_loss(loss: float, widget: widgets.FloatProgress, name: str = \"Loss\"):\n",
    "    widget.value = loss\n",
    "    widget.description = f\"{name}: {loss:.3f}\"\n",
    "\n",
    "    match loss:\n",
    "        case loss if loss < 0.25:\n",
    "            widget.style = {\"bar_color\": \"green\"}\n",
    "        case loss if loss < 0.50:\n",
    "            widget.style = {\"bar_color\": \"orange\"}\n",
    "        case loss if loss < 0.75:\n",
    "            widget.style = {\"bar_color\": \"red\"}\n",
    "        case _:\n",
    "            widget.style = {\"bar_color\": \"darkred\"}\n",
    "\n",
    "class Counter:\n",
    "\n",
    "    def __init__(self, name: str = \"Counter: {count}\", *, show: bool = True) -> None:\n",
    "        self.name = name\n",
    "        self.tmpl = Formatter().parse(name)\n",
    "        placerholders = [field_name for _, field_name, _, _ in self.tmpl if field_name]\n",
    "        if \"count\" not in placerholders:\n",
    "            raise ValueError(\"The format string 'name' must contain a 'count' placeholder.\")\n",
    "        self.count: int | float = 0\n",
    "        self.widget = widgets.Label(\n",
    "            value=\"Iterations: 0\"\n",
    "        )\n",
    "\n",
    "        if show:\n",
    "            display(self.widget)\n",
    "    \n",
    "    def update(self, count: int | float):\n",
    "        self.count += count\n",
    "        self.widget.value = self.name.format(count=self.count)\n",
    "\n",
    "    def set(self, count: int | float):\n",
    "        self.widget.value = self.name.format(count=count)\n",
    "\n",
    "    def clear(self):\n",
    "        self.count = 0\n",
    "        self.widget.value = self.name.format(count=self.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51610b",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420cdaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import cast\n",
    "\n",
    "from src.data.mmarco import mMARCOBatch\n",
    "from IPython.display import display\n",
    "\n",
    "def tokenize(batch: mMARCOBatch):\n",
    "    return model.tokenize(\n",
    "        batch[\"queries\"],  # pyright: ignore[reportGeneralTypeIssues]\n",
    "        batch[\"candidates\"],  # pyright: ignore[reportGeneralTypeIssues]\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "\n",
    "start = datetime.now()\n",
    "print(f\"Starting at {start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "batch_counter = Counter(\"Batches: {count}\", show=False)\n",
    "iter_counter = Counter(\"Iterations: {count}\", show=False)\n",
    "deviation_label = Counter(\"σ: {count}\", show=False)\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([progress_widget, batch_counter.widget, iter_counter.widget]),\n",
    "    widgets.HBox([training_loss_widget, deviation_label.widget, val_loss_widget]),\n",
    "]))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_losses = [[] for _ in range(epochs)]\n",
    "val_losses = [[] for _ in range(epochs)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # --- training loop ---\n",
    "    model.train()\n",
    "    for batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = cast(mMARCOBatch, batch)    \n",
    "        input = tokenize(batch)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type != \"cpu\")):\n",
    "            logits = model.forward(\n",
    "                input_ids=input[\"input_ids\"],  # type: ignore\n",
    "                attention_mask=input[\"attention_mask\"],  # type: ignore\n",
    "                token_type_ids=input[\"token_type_ids\"],  # type: ignore\n",
    "            ).squeeze(-1)\n",
    "            loss = criterion(logits, batch[\"labels\"].float().to(device))\n",
    "            train_losses[epoch].append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_loss(loss.item(), training_loss_widget, \"Train\")\n",
    "        batch_counter.update(1)\n",
    "        iter_counter.update(len(batch[\"queries\"]))\n",
    "\n",
    "        deviation_label.set(\n",
    "            torch.tensor(train_losses[epoch]).std(unbiased=False).item()\n",
    "        )\n",
    "\n",
    "    # --- validation loop ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            batch = cast(mMARCOBatch, batch)    \n",
    "            input = tokenize(batch)\n",
    "            logits = model.forward(  # type: ignore\n",
    "                input_ids=input[\"input_ids\"],  # type: ignore\n",
    "                attention_mask=input[\"attention_mask\"],  # type: ignore\n",
    "                token_type_ids=input[\"token_type_ids\"],  # type: ignore\n",
    "            ).squeeze(-1)\n",
    "            loss = criterion(logits, batch[\"labels\"].float().to(device))\n",
    "\n",
    "            val_losses[epoch].append(loss.item())\n",
    "            update_loss(loss.item(), val_loss_widget, \"Val\")\n",
    "\n",
    "\n",
    "    update_progress(epoch)\n",
    "\n",
    "    break\n",
    "        \n",
    "    \n",
    "model.eval()\n",
    "stop = datetime.now()\n",
    "elapsed = stop - start\n",
    "print(f\"Stopping at {stop.strftime('%Y-%m-%d %H:%M:%S')}. Elapsed time: {elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc4757",
   "metadata": {},
   "source": [
    "### Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39683be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bert_mmarco.pth\")\n",
    "\n",
    "# load with:\n",
    "# model = CrossEncoderBERT()\n",
    "# model.load_state_dict(torch.load(\"model_state.pth\"))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fad31",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4847ae1",
   "metadata": {},
   "source": [
    "During training and validation, the loss was collected for each batch.\n",
    "However, I am plotting the loss here at epoch-level. The losses are\n",
    "aggregated by epoch and a deviation is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Sequence\n",
    "\n",
    "def _summarize(loss_matrix: Sequence[Sequence[float]]):\n",
    "    \"\"\"Helper to summarize a matrix of losses (list of lists) into means, stds, and counts per epoch.\"\"\"\n",
    "    means: list[float] = []\n",
    "    stds: list[float] = []\n",
    "    counts: list[int] = []\n",
    "    for losses in loss_matrix:\n",
    "        if len(losses) == 0:\n",
    "            means.append(float(\"nan\"))\n",
    "            stds.append(float(\"nan\"))\n",
    "            counts.append(0)\n",
    "        else:\n",
    "            arr = np.asarray(losses, dtype=float)\n",
    "            means.append(float(arr.mean()))\n",
    "            stds.append(float(arr.std(ddof=0)))  # population std for stability\n",
    "            counts.append(len(arr))\n",
    "    return means, stds, counts\n",
    "\n",
    "train_means, train_stds, train_counts = _summarize(train_losses)\n",
    "val_means, val_stds, val_counts = _summarize(val_losses)\n",
    "\n",
    "epochs_axis = np.arange(1, len(train_losses) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5), dpi=110)\n",
    "\n",
    "# Plot lines\n",
    "ax.plot(epochs_axis, train_means, marker='o', label='Train Mean Loss', color='C0')\n",
    "ax.plot(epochs_axis, val_means, marker='o', label='Val Mean Loss', color='C1')\n",
    "\n",
    "# Std deviation bands (ignore NaNs automatically by masking)\n",
    "train_upper = np.array(train_means) + np.array(train_stds)\n",
    "train_lower = np.array(train_means) - np.array(train_stds)\n",
    "val_upper = np.array(val_means) + np.array(val_stds)\n",
    "val_lower = np.array(val_means) - np.array(val_stds)\n",
    "\n",
    "ax.fill_between(epochs_axis, train_lower, train_upper, color='C0', alpha=0.20, linewidth=0)\n",
    "ax.fill_between(epochs_axis, val_lower, val_upper, color='C1', alpha=0.20, linewidth=0)\n",
    "\n",
    "# Highlight best (lowest) validation epoch\n",
    "if any(not math.isnan(v) for v in val_means):\n",
    "    best_epoch_idx = int(np.nanargmin(val_means))\n",
    "    ax.scatter(epochs_axis[best_epoch_idx], val_means[best_epoch_idx], s=120, color='C1', edgecolor='k', zorder=5)\n",
    "    ax.annotate(\n",
    "        f\"Best Val\\nEpoch {best_epoch_idx+1}\\n{val_means[best_epoch_idx]:.4f}\",\n",
    "        (epochs_axis[best_epoch_idx], val_means[best_epoch_idx]),\n",
    "        textcoords=\"offset points\", xytext=(10, -5), ha='left', va='top', fontsize=9,\n",
    "        bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='C1', alpha=0.8)\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Binary Cross-Entropy Loss')\n",
    "ax.set_title('Training vs Validation Loss')\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend()\n",
    "ax.margins(x=0.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Structured summary printout\n",
    "for i, (tr_m, tr_s, tr_c, va_m, va_s, va_c) in enumerate(zip(train_means, train_stds, train_counts, val_means, val_stds, val_counts), start=1):\n",
    "    print(f\"Epoch {i:02d} | Train: {tr_m:.4f} ± {tr_s:.4f} (n={tr_c}) | Val: {va_m:.4f} ± {va_s:.4f} (n={va_c})\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41233a25",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    # TODO: also perform accuracy and other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8053e502",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(query: str, candidates: list[str]) -> list[tuple[str, float]]:\n",
    "    model.eval()\n",
    "    inputs = model.tokenize([query]*len(candidates), list(candidates)).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model.forward(\n",
    "            input_ids=inputs[\"input_ids\"],  # type: ignore\n",
    "            attention_mask=inputs[\"attention_mask\"],  # type: ignore\n",
    "            token_type_ids=inputs[\"token_type_ids\"],  # type: ignore\n",
    "        )\n",
    "    scores = torch.sigmoid(logits).squeeze(-1).tolist()\n",
    "    return sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b941da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What is the capital of France?\"\n",
    "test_candidates = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Marseille is a city in France.\",\n",
    "    \"Lyon is known for its cuisine.\",\n",
    "    \"France is in Europe.\",\n",
    "    \"Macron is the president of France.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Madrid is the capital of Spain.\", \n",
    "    \"Rome is the capital of Italy.\"\n",
    "]\n",
    "\n",
    "ranked_results = rank(test_query, test_candidates)\n",
    "\n",
    "for candidate, score in ranked_results:\n",
    "    print(f\"Score: {score:.4f} - Candidate: {candidate}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
